# ğŸ¤– Ollama Installation Guide - For AI Features

**Quick Guide to Install Ollama and Enable AI Assistant**

---

## ğŸ“‹ What is Ollama?

Ollama is a **free, open-source tool** that runs large language models (LLMs) locally on your Mac/Linux/Windows machine. It's the engine that powers your AI Assistant features.

**Key Features:**
- ğŸ†“ Completely free and open-source
- ğŸ”’ 100% offline and private
- ğŸ’» Runs locally on your machine
- âš¡ Fast and efficient
- ğŸ¯ Easy to use

---

## ğŸš€ Installation (macOS)

### Option 1: Homebrew (Recommended)
```bash
brew install ollama
```

### Option 2: Official Installer
1. Download from: https://ollama.com/download
2. Install the .dmg file
3. Done!

---

## ğŸš€ Installation (Linux)

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

---

## ğŸš€ Installation (Windows)

1. Download from: https://ollama.com/download
2. Run the installer
3. Done!

---

## ğŸ¯ Quick Start (3 Steps)

### Step 1: Start Ollama Service
```bash
ollama serve
```

**Leave this terminal open!** This runs Ollama in the background.

### Step 2: Pull an AI Model (Choose One)

**Recommended: Mistral 7B (4GB)**
```bash
ollama pull mistral:7b
```
- Best balance of speed and quality
- Works great for business queries
- 4GB download

**Alternative: Llama 3 (4.7GB)**
```bash
ollama pull llama3:8b
```
- More powerful, slightly slower
- Better for complex reasoning
- 4.7GB download

**Lightweight: Phi 3 Mini (2.3GB)**
```bash
ollama pull phi3:mini
```
- Fastest, smallest model
- Good for simple queries
- 2.3GB download

### Step 3: Verify Installation
```bash
curl http://localhost:11434/api/tags
```

**Expected Response:**
```json
{
  "models": [
    {
      "name": "mistral:7b",
      "size": 4109865159,
      "digest": "...",
      "modified_at": "2024-10-26T..."
    }
  ]
}
```

âœ… **You're ready!** AI Assistant will now work.

---

## ğŸ”§ Configuration (Optional)

### Change AI Model in Backend

Edit `my-backend/.env`:
```bash
# Change this line to use a different model
OLLAMA_MODEL=mistral:7b
# or
OLLAMA_MODEL=llama3:8b
# or
OLLAMA_MODEL=phi3:mini
```

Restart your backend server after changing.

---

## ğŸ§ª Test Your Installation

### 1. Test Ollama Directly
```bash
ollama run mistral:7b "What is 2+2?"
```

**Expected:** AI should respond with "4" and explanation.

### 2. Test Health Endpoint
```bash
# Start your backend first
cd my-backend
npm start

# In another terminal
curl http://localhost:5000/api/ai/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "ollama": {
    "available": true,  // âœ… Should be true now!
    "model": "mistral:7b",
    "baseUrl": "http://localhost:11434"
  }
}
```

### 3. Test AI Chat
```bash
# Get JWT token from login
TOKEN="your-jwt-token"

# Ask AI a question
curl -X POST http://localhost:5000/api/ai/query \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are my total sales for today?",
    "sessionId": "test-session-123"
  }'
```

**Expected:** AI should respond with insights based on your data!

---

## ğŸ¯ What Will Work After Installation

âœ… **AI Chat Queries** - Ask questions in natural language  
âœ… **Natural Language SQL** - Convert questions to database queries  
âœ… **Automated Reports** - Daily reports generated by AI  
âœ… **Sales Predictions** - AI-powered forecasting  
âœ… **Analytics Insights** - Intelligent business insights  
âœ… **Data Summarization** - Automatic summaries of your data  

---

## ğŸ”§ Troubleshooting

### Issue: "Ollama not found"
**Solution:**
```bash
# Check if installed
which ollama

# If not found, install again
brew install ollama  # macOS
```

### Issue: "Connection refused"
**Solution:**
```bash
# Make sure Ollama is running
ollama serve &

# Check if it's listening
curl http://localhost:11434/api/tags
```

### Issue: "Model not found"
**Solution:**
```bash
# Pull the model
ollama pull mistral:7b

# List available models
ollama list
```

### Issue: "Slow responses"
**Solutions:**
1. Use a smaller model: `ollama pull phi3:mini`
2. Upgrade RAM (8GB minimum recommended)
3. Close other applications
4. Use SSD storage

---

## ğŸ›ï¸ Advanced Configuration

### Run Ollama on Startup (macOS)

Create `~/Library/LaunchAgents/com.ollama.serve.plist`:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.ollama.serve</string>
    <key>ProgramArguments</key>
    <array>
        <string>/opt/homebrew/bin/ollama</string>
        <string>serve</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
</dict>
</plist>
```

Load it:
```bash
launchctl load ~/Library/LaunchAgents/com.ollama.serve.plist
```

### Run Ollama on Startup (Linux - systemd)

Create `/etc/systemd/system/ollama.service`:
```ini
[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
User=youruser
ExecStart=/usr/local/bin/ollama serve
Restart=always

[Install]
WantedBy=multi-user.target
```

Enable and start:
```bash
sudo systemctl enable ollama
sudo systemctl start ollama
```

---

## ğŸ“Š Model Comparison

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| **mistral:7b** | 4GB | âš¡âš¡âš¡ | â­â­â­â­ | **Recommended** - Best balance |
| **llama3:8b** | 4.7GB | âš¡âš¡ | â­â­â­â­â­ | Complex reasoning |
| **phi3:mini** | 2.3GB | âš¡âš¡âš¡âš¡ | â­â­â­ | Quick queries |
| **llama3:70b** | 40GB | âš¡ | â­â­â­â­â­ | Advanced (needs 64GB RAM) |

---

## ğŸ’¡ Tips & Best Practices

### 1. Keep Ollama Running
- Run `ollama serve &` in background
- Or set up auto-start (see advanced config)

### 2. Pull Models Ahead of Time
```bash
# Download during off-hours
ollama pull mistral:7b
ollama pull llama3:8b
```

### 3. Monitor Performance
```bash
# Check Ollama logs
tail -f ~/.ollama/logs/server.log

# Check resource usage
top -pid $(pgrep ollama)
```

### 4. Regular Updates
```bash
# Update Ollama
brew upgrade ollama  # macOS

# Update models
ollama pull mistral:7b
```

---

## ğŸ‰ You're All Set!

After installation:
1. âœ… Ollama is running
2. âœ… Model is downloaded
3. âœ… Health check passes
4. ğŸš€ AI features are active!

**Test your AI Assistant:**
- Go to `/common/ai-assistant` in your app
- Ask: "What are my total sales today?"
- Watch AI analyze your data! ğŸŠ

---

## ğŸ“š Learn More

- **Ollama Documentation:** https://ollama.com/docs
- **Available Models:** https://ollama.com/library
- **Community:** https://github.com/ollama/ollama

---

**Need Help?** Check the logs:
```bash
# Backend logs
tail -f my-backend/logs/server.log

# Ollama logs
tail -f ~/.ollama/logs/server.log
```

---

**Installation Guide Last Updated:** October 26, 2024
