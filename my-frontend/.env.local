# Frontend local runtime variables
# Backend URL (used by Next.js API routes for proxying)
NEXT_PUBLIC_API_URL=http://localhost:3001

# Use Next.js API proxy instead of direct backend calls (eliminates CORS)
# Set to 'true' only if you want to bypass Next.js proxy
NEXT_PUBLIC_DIRECT_BACKEND=false

# Database connection for Prisma
DATABASE_URL="postgresql://postgres@localhost:5432/BISMAN"

## Ollama - Local development
# Browser should use the server relay; keep NEXT_PUBLIC_OLLAMA_URL unset/commented
OLLAMA_URL=http://localhost:11434
# NEXT_PUBLIC_OLLAMA_URL=http://localhost:11434
# Optional alias
OLLAMA_HOST=http://localhost:11434

## (Old) ngrok example – leave commented unless tunneling directly to Ollama
# OLLAMA_URL=https://your-ngrok-subdomain.ngrok-free.dev
# OLLAMA_HOST=https://your-ngrok-subdomain.ngrok-free.dev

# Public env vars for browser-side access (keep URL unset to force proxy)
# NEXT_PUBLIC_OLLAMA_URL=https://ned-throatless-secondarily.ngrok-free.dev
NEXT_PUBLIC_OLLAMA_MODEL=tinyllama:latest
OLLAMA_MODEL=tinyllama:latest

# Force server relay and use generate-only for older Ollama
OLLAMA_FORCE_PROXY=true
OLLAMA_PREFER_GENERATE=true

## Production (Railway) — set these on the service, not here
# OLLAMA_URL=http://ollama.railway.internal:11434
# OLLAMA_FORCE_PROXY=true
# OLLAMA_PREFER_GENERATE=true
# OLLAMA_MODEL=tinyllama:latest
# NEXT_PUBLIC_OLLAMA_MODEL=tinyllama:latest
# (Do NOT set NEXT_PUBLIC_OLLAMA_URL)

# Central AI (Railway Open WebUI or proxy)
AI_BASE_URL=https://open-webui-production-6e46.up.railway.app
AI_DEFAULT_MODEL=llama3
AI_KEY=
